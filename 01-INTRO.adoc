= Data Integration

== Data Fragmentation

Databases and the analytics tools that sit on top of them allow us to get answers to questions based on a huge amount of data, which is awesome. However, for this to be feasible there are three key assumptions:

1. the data is all in a single schema;
1. the data doesn't contain duplicate records;
1. the data is up to date and has no conflicts or quality issues.

In reality though, this is not often the case, and the data that needs to be analysed has to come from multiple sources where the databases have been created entirely independently, and there is little to no opportunity to homogenise these data sources for either technical or commercial reasons. These different data sources can:

1. use different data models;
1. use diferent schemas;
1. describe the same entity;
1. have conflicting data about the same entity;
1. have different query interfaces and languages to access the data.

In my previous start-up Kusiri, I saw this problem first-hand in large enterprises. In banks it tended to be mainly internal data - both from having acquired other businesses, and from seperate IT initatives that created data silos. In Professional Services companies, it tended to be mostly external, needing to leverage many external data sources that were all heterogeneous in a consistant way in order to do research.

Data Integration attempts to find solutions to this real-world problem of Data Fragmentation.

== Data Integration

One definition that follows for data integration is that:

> Data integration is the process of consolidating data from a set of heterogenous data sources into a homogenous data set or view on the data.

It is, in effect, offering the fragmented data up as if it were a single database for query.

=== The scale of challenge

Data integration is a hard problem, it is "AI Complete", according to Alon Halevy*, meaning that completely automated solutions are unlikely. The reasons for this are three-fold:

1. System level: different platforms
1. Logical: schema and data homogenity
1. Social: locating data, getting access to data

=== Solving data integration

To break this out more, there are a number of properties that we want from the homogenous database:

1. accurately and completly represent the content of all data sources;
1. represent the entities using a single data model and schema;
1. not contain duplicate records representing an entity (unless expected);
1. resolve any conflicting data about entities.

In order to achieve this, data integration needs to be able to resolve the different types of heterogenity between different data sources.

== Data Source Heterogenity

There are five different types of heterogenity that we need to solve for in the data:

1. Data Access
2. Data Encoding
3. Data Model
4. Data Structure
5. Data Semantics

As we will see, the application of standards in each of these areas can reduce or eliminate the heterogenity in these areas.

=== Access Heterogenity

There are many different layers to be being able to access data, and we must bridge these layers in order to consolidate our data.

.Access Levels
[options="header"]
|======
|Level|Examples
|Protocol|HTTP/REST, JDBC, SOAP
|Data Exchange Format|JSON, Protobuf, Parquet
.3+|Query Language|Full Languages, e.g. SQL
Predefined; Canned queries, e.g. web forms/APIs, 
|Canned queries, e.g. web forms/APIs
|Data dumps
|Restrictions|Thoughput, cost, rights
|======

Note that although databases may hae a full language to construct any desired query on the data, in reality pre-defined indexes may be required for such.

=== Encoding Heterogenity

This is the difference in the encoding of the data and meta-data. This often does not apply when using some standardised access methods, such as JDBC, where any underlying differences in how the data is encoded is factored out by the access layer. You can see that it would apply however in data dumps, for example:

.Encoding Examples
[options="header"]
|======
|Type|Examples
|Text|Different binary encodings of character sets, e.g. UTF-8
|Numbers|Different encoding of numbers, e.g. fixed length, varint, little vs big endian
|Delimiter|Which character is the C of CSV
|======

In practice, this is the easiest bridge to build between data sources due to there being good support for most different standards and charsets in most platforms and programming languages.

=== Data Model Heterogenity

This is the difference in the type of Data Model that is used to represent data. This is importantly different from the schema, which describes how we model the attributes of entities, but rather this is how we model the entities themselves and the relationships between them.

.Data Model Classes
[options="header"]
|======
|Class|Relationships|Examples
|Relational|Row-to-Row|RDBMS, e.g. MySQL. Note that a "cell" can itself have hierarchical data stored within it, but the relationships are modelled from tow to row.
|Hierarchical|Nested data|XML/JSON; Document Stores where the data is stored inline "nested" rather than normalized into multiple tables.
|Object Oriented|Inheritance, Object-to-Object|Object Oriented databases
|Graph|Vertex-to-Vertex, Directed|RDF, Graph databases
|Relational++|Row-to-Row, Inheritance, Nested|PostgreSQL: a "cell" can itself have hierarchical data stored within it, there exists table inheritance, but the relationships are modelled between rows
|======

=== Structural Heterogenity

Structural heterogenity is the difference in the data structures, such as tables, used to model the same data. This can include differences in:

1. Alternative modelling
1. Normalization
1. Nesting vs Foreign Key

==== Alternative modelling

Consider how you capture data for dogs and cats. Do you have a dogs table and a cats table? Do you have an animals table with an `species` column? Do you have an `is_cat` column if you know it's only cats and dogs? There are many ways to model the same data.

==== Nomalization

Normalization refers to the level of different tables you break things out to. For example, let's say that each person has a list of hobbies. In a RDBMS you could have a person and a hobby table, and a join table to track who has what hobbies. Alternatively, you could just put the string values in a "hobbies" array within the person row. Or you could remove the join table and track a list of ids within the person row.

Higher levels of normalization are generally thought to be "cleaner" representations of data, whereas you can get better real-world performance from some denormalization due to the disk/memory access required to fetch a person and their hobbies.

==== Nesting vs Foreign Key

Similar to normalization, this is a question of whether the data is stored in a normalized form, or nested within.

For example, let's suppose you have people and their pets. Each pet only ever has one owner. In PostgreSQL you could either have a people and pet table, or alternatively you could have a `pets` column within the person table that stored a JSON blob that contained a list of pets, their names, ages, and so on.

=== Semantic Heterogenity

Semantic heterogenity is the differences in the meaning of the schema and data of the different data sources.

==== Data heterogenity

.Data heterogenity
[options="header"]
|======
|Class|Examples
|Numeric|Can be power difference (e.g. GB vs MB), currency (e.g. USD vs GBP), or unit type (e.g. km vs miles)
|Enum|Different representations in textual (e.g. Male vs M), or numeric (e.g. 2 => Manager) encodings
.4+|Text|Synonyms (e.g. Street vs St.)
|Homonyms (e.g. transliteration of Arabic)
|Spellings (e.g. color vs colour)
|Conceptual, e.g. same name but different meaning
|Date/Time|Implicit timezone
|======

==== Schematic heterogenity

.Schematic heterogenity
[options="header"]
|======
|Class|Examples
|Attribute naming|Synonyms across sources, e.g. Last Name vs Surname
|Attribute composition|e.g. Name vs First Name & Last Name
|======


==== Object Identity

The same item, e.g. product, is often represented in multiple data source, or sometimes even by multiple unlinked records within the same Data Source.

The reason that we could have duplicates in a data sources could include:

* Human error, such as typos
* Lack of consistent global identifier (primary key)

==== Data conflicts

Multiple data sources can have different values for the same entity attribute. This could be decause of:

* Errors
* Old data in one source
* Disagreement - one source actually thinks that the value is different

In these cases there normally needs to be an automated way of deciding what the "source of truth" is for a given attribute.

= Web Data Integration

The Web is the largest repository of data in the known history of the universe.

It is clear from the definition of Data Integration, that if you view the World Wide Web as a data source, or indeed as many different data sources, the ability to bring the data together from different parts of the web into a single datastore is an extension of the Data Integration problem. 

Here the data is heteregenously available, and needs to be homogenised and consolidated in order to be leveraged.

== Sources of data on the Web

=== Data catalogues and marketplaces

Data catalogues and marketplaces collect and host data sets and their meta-data. Access to some data may be free, and others may be paid.

These data sets may be CSV files, spreadsheet formats, XML, JSON or even database dumps.

WARNING: These need to be checked and edited, I'm not sure Factual still operating like that

Some examples of sites offering public sector data sets are http://data.gov.uk[data.gov.uk], http://data.gov.us[data.gov.us] and http://publicdata.eu[publicdata.eu].

Commercial marketplaces include Factual, Dun & Brad Street, DataStreamX.

You can access a list of data catalogues at http://data.wu.ac.at/portalwatch/portallist[Portal Watch].

=== Web Applications and Sites

There are many web applications that publicly expose information, either through their HTML UI or HTTP APIs. This imformation might come in the form of:

* Tables
* Search results
* Lists of entities
* Entity "profile" pages

The data exposed through these UIs and APIs can be collected, but it is only a small slice or the data that is siloed by these companies, and the data that can be collected is restricted to whatever canned queries are available. Furthermore, the number of results that can be fetched can be lower than desired, as there may be artifical limits in place within the UI or API.

Some of the APIs may require authentication, which may place limits around usage.

==== Data freedom, and the rise of protectionist behaviour

WARNING: This section needs more thought

As companies have come to realise the value of Web Data Integration, some comapnies have taken a protectionist view on their own data and employed techniques and companies to try to limit the access of their publicly accessible data by third parties in order to try to reduce competition, or give themselves a market advantage.

This is certainly not in line with the free market approach that most of these companies espouse, rather the data is treated as trade secrets. 

While it is certainly the case that companies need to protect themselves against malicious behvaiour, and the potential cost that non-human access of their data may cost, they must start to open their eyes and realise that all their competitors will do the same thing. A protectionist web data arms race will serve no-one apart from those producing the weaponry.

It is certainly my hope that over time these companies will come to see that gathering of publicly available data is not something you do yourself in secret, which castigating others for the same.

=== The Semantic Web

The concept of the "Semantic Web" extends the web conceptually by using HTTP URIs (URLs) to uniquely identify both entities and the types of relationships between entities.

This type of data is represented in RDF, normally as "triples", where each triple is a subject-property-object, e.g. _<Matthew Painter> <graduated from> <Cambridge University>_.

What is actually at the HTTP URL for an entity can vary. (TODO)

SPARQL is the SQL-like query language that is standardized for querying Linked Data.

RDF and SPARQL has a low low commercial adoption rate. (TODO: demonstrate this.) Some of the areas where RDF is used are Life Sciences and Libraries.

=== HTML Embedded Structured Data

Data can be embedded in HTML pages, and while that can be very useful, it is often incomplete. As such it can form a solid source of _some_ of the data you are wishing to collect.

==== Linked Data

Google, Yahoo, Bing, and Yandex all had the same problem - the fact the web is a set of unstructured HTML pages and other documents really started to hamper their ability to answer questions for their users that went beyond pointing them to a web page. To improve their customer experience, they needed better, structured data on each page that followed a consistent schema. You can see that they were trying to solve Data Integration in general for the web!

So, in order to try to get access to embedded structured data available on the web pages they were crawling, they came together in 2011 to try to create a single, homogenous schema for representing a lot of data on the web, http://schema.org[schema.org]. There are currently over 600 entity types that can be represented by schema.org, and as of 2014 over 5 million websites provide schema.org data.

It is worth pointing out that even with Google's very deep technical pockets they did not start out looking at solving this programatically, although perhaps now they have a lot of training data this is in play.

Along with schema.org came new, simpler ways of embeddeding RDF data in HTML: RDFa, Microdata and JSON-LD.

The adoption for this has been marked, as the graph below shows:

(TODO: Add graph)

The reason behind this is simple - better search listings due to better structured data gave better conversion, and so there was a commercial driver to ensure that in the SEO arms race no-one was overtaken: they managed to create a positive feedback loop.

TODO: Add graphic showing Google search results

It is worth pointing out that companies can get so large they are seemingly immune to such SEO drivers, for example Amazon has no such embedded linked data.

===== RDFa

RDFa was first proposed in 2004, and was recommended by W3C in 2008. Its use has been superceeded by Microdata and JSON-LD, both of which are simpler formats.

TODO: Add in example

===== Microdata

Microdata is an alternative, less verbose, markup than RDFa:

TODO: Add in example

It was proposed in 2009 by the WHATWG group as part of the HTML5 specification and standardization process.

===== OpenGraph

WARNING: Is OpenGraph really RDFa?

OpenGraph is a proprietary Facebook schema and embeddeding mechanism that allows companies to define how their pages look when consumed within Facebook. Facebook rolled it out in 2010.

TODO: add link to opengraph schema, add example

==== Microformats

Microformats date back to 2003, and comprise a small set of fixed formats:

.Mircoformats
[options="header"]
|======
|Format|Entities
|hcard|people, companies, organizations, places
|XFN|relationships between people
|hCalendar|Calendaring and events
|hListing|Classifieds
|hReview|reviews of products, businesses, events
|======

The main problem with microformats was their lack of extensibility. They exist now, but really are deprecated, and their use has been superceeded by schema.org embedded data.

==== Adoption

As of 2017, approximately 38% of HTML pages had embedded structured data, and approximately 28% of domains have embedded data. Clearly there is a lot of structured data already available on the web. (TODO: Common crawl - add link)

Over 5 million sites had already embedded schema.org data in 2014.

TODO: add in adoption of e-commerce, etc.

=== Common Crawl

The Common Crawl project crawls approximately 3 billion HTML pages every year. Many web sites have a selection of pages, but the aim is not to crawl any site completely. 

The Web Data Commons project extracts all the HTML embedded in the common crawl corpus, analyzes it and makes it available for download.

TODO: Add some links, cool stats, graphs...

Web Data Commons is a very good way of getting some representative data where you need a sample across many sites, or for a particular entity type, but should not be considered for deep collection purposes due to its incompleteness.

=== HTML Tables

There are hundreds of millions of high quality HTML data tables on the web, many of which are within Wikipedia.

TODO: add in graphics from study, read study

Part of the issue here is working out the schemas for the data, clearning the data, etc.

==== Web Data Commons - Web Tables Corpus

The Web Tables Corpus is a large public corpus of 90 million relational Web Tables.

These tables were filtered out of a 10.2 billion raw tables from Common Crawl.

TODO: Look for newer stats, read up about this

==== Google Table Search

TODO:

=== Wikipedia

Several projects exist to try to capture the content of Wikipedia in a structured format.

==== DBpedia

DBpedia decrises over 6 million things, of which over 5 million are classified in a consistent schema using 760 classes and 2739 properties, including:

* 1.5M people
* 1M places
* 250k organizations

Altogether there are 13 billion RDF triples, of which 1.7 billion are English, 29 million are links to external sites, and 50 million are external links to other data sets.

==== Yago/Wikidata

TODO:

=== Knowledge graphs as Web Data Integration

A knowledge graph is a large cross-doman knowledge base which aims to cover all entities in the world. They tend to be proprietary and based upon RDF technology. 

They are great examples of large-scale Web Data Integration as they combine data from many of the sources into a single database:

1. Wikipedia
2. Open license sources
** CIA World Factbook
** MusicBrainz
** ...
3. Commercial Third Party Data
4. HTML Embedded Structured Data

A very large amount of effort is spent on data integration and manual curation of data.

==== Google Knowledge Graph

Google acquired Freebase in 20??, that attempted to be an open Knowledge Graph. Google then started developement of the Google Knowledge Graph based upon this in 2012. As of 2012 there are 570 million objects described by over 18 billion triples, with 1,500 classes and 35,000 properties in the taxonomy.

When you are searching, Google leverages this Knowledge graph to provide you structured data as part of your results, so you don't have to leave their application to get your answers, both by giving you supplemental data visually and also by directly using the facts to answer questions such as "compare the Eiffel Tower and the Empire State Building".

TODO: Add in image

Due to this, content optimisation for companies of other web data, such as the company Wikipedia entry, becomes more important.

TODO: add in example of searching for import.io or something

Behind the scenes Google's Hummingbird algorithm (2013) uses knowledge graph for weights in ranking its search results also.

==== Microsoft Satori Knowledge Base

This was revealed to the public in 2013.

== WDI: Databases

You will recall that we identified that we need to standardise:

1. Access
2. Encoding
3. Data Model
4. Data Structure
5. Schema

There are two extremes of model in Data Integration - one where you consolidate the data into a database, and one when you leave the data in its data stores, but provide a translation layer on top. The latter is really only useful if you cannot realistically pull all of the required data into the same place - for example, using the Bing web search API.

Therefore, when talking about web data, it really only makes sense to pull the data into a database, because the APIs are access methods are so restrictive that you will not be able to carry out the analytics you require on the data.

So, now we know that we need a database to consolidate the data into, let's look at some of our options.

=== Databases

==== RDF stores

RDF stores have incredibly low adoption in the wider technical community, and I could not recommend using one unless you have either the budget or the people to take one to production.

TODO: More information

==== Relational Databases

Relational (OLTP) databases organise the data on disk in rows. This makes it very efficient to be able to read and write the values generally in a single row, but it becomes much more expensive to be able to carry out analytics on a particular column. To overcome this, you can use covering indexes such that for a particular query only the data from the B-Tree index has to be read, which is much more efficient. However, this requires knowing all of the data access patterns up front, and also takes more space as the data is stored twice. This, you can imagine, is problematic for situations where the team who is creating the queries do not want to be having to wait for indexes to be created.

It's for this reason that generally relational databases are not recommended for general analytical workloads.

==== Document stores

Similarly document stores tend to have to parse the full object in JSON/BSON order to carry out analysis, unless the object is indexed. This has the same issues as relational databases for ad-doc analytical queries.

==== Columnar Databases

Columar databases are gnerally OLAP databases or search indexes that tend to store the data in pages - groups of rows - on disk, where the pages are partitioned by some (composite) key, such that if, for example, you want to get all the events in a specific time window, the corresponding data pages to check can be located, hence reducing the amount of data to be scanned. Within each page, the data is stored in rows, and may be encoded in a way that makes the bytes that need to be scanned again decrease, e.g. a byte dictionary may be used for string enum values. Therefore in order to calculate an aggregate on a column, it is reading contiguous disk and making most efficient use of the CPU cache buffers, or to filter a column, within each page a bitset representing the rows can be used and each column can be scanned to work out the rows to materialize from the columnar encoding. Generally a map-reduce style pattern is used for the query, where there is a fan out in order to parallelize the querying of the pages, and a fan-in to combine the results back into a single result set.

As such, the efficiency of such a database is really determined by the amount of data that needs to be scanned, and how contiguous the data being scanned is. Queries need to be constructed with this in mind, otherwise you can end up scanning all the data you have in your table.

Columnar databases often are append-only: they do not support mutating old data. Or when they do, e.g. Redshift, the update process cannot be used in bulk due to its inefficiency at mutating the columnar data structures. For this reason, if you have a part of your integrated data where data may mutate often, or due to user input, it is recommended to use a relational database for that data, and export the mutable data out on some kind of schedule or event basis into immutable form. Careful consideration is needed here in design to ensure that it is understood that there may be some lag from the mutation of the data to that being mirrored into the 

===== Search engines

THe main example here is Lucene, and those engines that use it the build search clusters such as Elastic Search or Solr. These systems can give very good latency of results, but generally can be hard to scale the indexes up to support many entity types.

With this model, the storage capacity is directly linked to each node in the cluster, as it the CPU and memory - that is, the CPU, memory and storage are all tightly coupled together. This makes indexes very difficult to scale, as you cannot elastically scale the throughput independently of the storage: you will need to physically add more disks to a node, or more RAM/CPUs.

===== Data Warehouses

Data Warehouses, such as AWS Redshift, are OLAP databases that are sharded, and each shard maintains its own slice of data. Again, with this model, the storage capacity is directly linked to each node, and has the previously discussed scaling issues.

===== Data Lakes

Data Lakes services, such as AWS Redshift Spectrum or Import.io Tables, are OLAP databases where the data is stored in files off-node. Modern columnar file formats such as Parquet are used to store the data in a columnar fashion. They use a massively-multiprocessing* model where the processing layer can be scaled depending on the size of the query based upon how many partitions are required and the size of those partitions. This overcomes the availity to independently scale the storage to the throughput. As such you can append data into your lake forever, and if you are only querying the last 30 days, your actual processing costs will not increase.

=== Homogenisation strategy

So, on balance, seems like the best option for general queries today is to use a Data Lake, like AWS Redshift Spectrum.

Let us come back to what we need to standardise with this new information.

.Homogenisation solution
[options="header"]
|======
|Item|Solution
|Access|SQL
|Encoding|JSON => Parquet
|Data Model|Relational with denormalization/nesting where sensible
|Data Structure|Standardized on a per-project basis
|Schema|Standardized on a per-project basis using JSON Schema
|======

The one Gotcha here is large integers, which are not natively supported by Javascript. 

WARNING: Are large integers in the JSON spec? 

However, if parsing JSON into Java, you can work around this?

This gives us a full toolchain to be able to carry out our Web Data Integration. So let's actually see what would look like front to back.

== WDI In Practice

Let's break down what a WDI project looks like:

1. Problem statement
1. Data source identification
1. Schema definition
1. Query definition
1. Data source mapping

=== Problem statement

Start off with a problem statement: what question(s) are you trying to answer using data? WDI is a business driven activity, there should be a clear value proposition from the project. Of course, you won't know all the questions at the start, but you should try to define enough questions to validate that you are collecting the correct data.

=== Data source identification

You should now be able to determine where you want to get this data from. This is sometimes driven by business requirements (competitors), and sometimes by technical considerations. 

=== Schema definition

You must now define the schema that you want to end up with. To do this, you need to write the JSON schema required, although personally I prefer to write it in YAML and then compile it to JSON, because:

1. YAML is easier to read
1. YAML is easier to write
1. YAML can be commented

The schema should really be driven from the business problem you are trying to collect data to answer, but potentially with an eye on what the next question might be. It's not possible to go back and collect histroic data, so it's better to try to get the coverage of data you think you will need for the project. It is a bit waterfall by necessity, however you can still be lean and go back and add and change fields. However once you start collecting data the cost of change becomes much higher.

=== Query definition

Now you have your schema defined, you should be able to write down the SQL queries you would execute in order to get the answers you want. This really is a vlidation exercise to check that the schema is correct, and once you start pressing forward you will be able to answer the questions with the data you have.

=== Data source mapping

Now you have the data sources and the schema, you should be able to map the schema to the data points that are on the data sources.



== The Data Integration Process

1. Data mapping
1. Schema mapping
1. Data collection
1. Data QA (during collection)
1. Data translation
1. Identity resolution
1. Data fusion
1. Data QA 2

TODO: Bing Liu - Wrapper induction using ML techniques